# app.py
# Streamlit: ì§€ì—­ë³„ ì˜ë£Œí–‰ìœ„(ì‹¬í‰ì›) Ã— ì¸êµ¬ì¦ê°(ì£¼ë¯¼ë“±ë¡) + ì‹œë„ ê²½ê³„ ì§€ë„(Choropleth)
# ì‹¤í–‰: streamlit run app.py

import re
import json
import requests
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px

st.set_page_config(page_title="ì§€ì—­ë³„ ì˜ë£Œí–‰ìœ„ Ã— ì¸êµ¬ì¦ê° ëŒ€ì‹œë³´ë“œ", layout="wide")


# -----------------------------
# CSV loader (auto-encoding)
# -----------------------------
def read_csv_from_path(path: str) -> pd.DataFrame:
    # ë¡œì»¬ íŒŒì¼ì€ ë³´í†µ utf-8-sig ë˜ëŠ” cp949ê°€ ë§ì•„ì„œ ë‘˜ ë‹¤ ì‹œë„
    for enc in ["utf-8-sig", "cp949", "euc-kr", "utf-8"]:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path)



def to_numeric_safe(s: pd.Series) -> pd.Series:
    return pd.to_numeric(s.astype(str).str.replace(",", "").str.strip(), errors="coerce")


def normalize_sido_from_pop(í–‰ì •êµ¬ì—­: str) -> str:
    """
    ì˜ˆ: 'ì„œìš¸íŠ¹ë³„ì‹œ  (1100000000)' -> 'ì„œìš¸'
    ì‹¬í‰ì› ì‹œë„ í‘œê¸°(ì„œìš¸, ë¶€ì‚°, ... ì¶©ë¶, ì¶©ë‚¨ ë“±)ì™€ ë§¤ì¹­ë˜ë„ë¡ ë‹¨ìˆœí™”
    """
    if pd.isna(í–‰ì •êµ¬ì—­):
        return np.nan

    name = re.sub(r"\s*\(.*?\)\s*", "", str(í–‰ì •êµ¬ì—­)).strip()
    name = re.sub(r"\s+", " ", name)

    # remove suffixes
    name = (
        name.replace("íŠ¹ë³„ì‹œ", "")
        .replace("ê´‘ì—­ì‹œ", "")
        .replace("íŠ¹ë³„ìì¹˜ì‹œ", "")
        .replace("íŠ¹ë³„ìì¹˜ë„", "")
        .replace("ìì¹˜ë„", "")
        .replace("ë„", "")
    ).strip()

    mapping = {
        "ì„œìš¸": "ì„œìš¸",
        "ë¶€ì‚°": "ë¶€ì‚°",
        "ëŒ€êµ¬": "ëŒ€êµ¬",
        "ì¸ì²œ": "ì¸ì²œ",
        "ê´‘ì£¼": "ê´‘ì£¼",
        "ëŒ€ì „": "ëŒ€ì „",
        "ìš¸ì‚°": "ìš¸ì‚°",
        "ì„¸ì¢…": "ì„¸ì¢…",
        "ê²½ê¸°": "ê²½ê¸°",
        "ê°•ì›": "ê°•ì›",
        "ì¶©ì²­ë¶": "ì¶©ë¶",
        "ì¶©ì²­ë‚¨": "ì¶©ë‚¨",
        "ì „ë¼ë¶": "ì „ë¶",
        "ì „ë¼ë‚¨": "ì „ë‚¨",
        "ê²½ìƒë¶": "ê²½ë¶",
        "ê²½ìƒë‚¨": "ê²½ë‚¨",
        "ì œì£¼": "ì œì£¼",
    }
    return mapping.get(name, name)


# -----------------------------
# GeoJSON (no file needed)
# -----------------------------
@st.cache_data(show_spinner=False)
def load_korea_sido_geojson():
    """
    ì‹œë„(1ë‹¨ê³„ í–‰ì •êµ¬ì—­) ê²½ê³„ GeoJSONì„ ì›¹ì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ë„¤íŠ¸ì›Œí¬ ì œí•œ/ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë˜ì§€ë©°, í˜¸ì¶œë¶€ì—ì„œ fallback ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    url = "https://simplemaps.com/static/svg/country/kr/admin1/kr.json"
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    return r.json()


# Simplemaps admin1 id â†” ì‹œë„ëª… ë§¤í•‘
GEO_ID_BY_SIDO = {
    "ì„œìš¸": "KR11",
    "ë¶€ì‚°": "KR26",
    "ëŒ€êµ¬": "KR27",
    "ì¸ì²œ": "KR28",
    "ê´‘ì£¼": "KR29",
    "ëŒ€ì „": "KR30",
    "ìš¸ì‚°": "KR31",
    "ê²½ê¸°": "KR41",
    "ê°•ì›": "KR42",
    "ì¶©ë¶": "KR43",
    "ì¶©ë‚¨": "KR44",
    "ì „ë¶": "KR45",
    "ì „ë‚¨": "KR46",
    "ê²½ë¶": "KR47",
    "ê²½ë‚¨": "KR48",
    "ì œì£¼": "KR49",
    "ì„¸ì¢…": "KR50",
}


# (fallbackìš©) ì‹œë„ ì¤‘ì‹¬ì  ì¢Œí‘œ(ëŒ€ëµ)
SIDO_CENTROIDS = {
    "ì„œìš¸": (37.5665, 126.9780),
    "ë¶€ì‚°": (35.1796, 129.0756),
    "ëŒ€êµ¬": (35.8714, 128.6014),
    "ì¸ì²œ": (37.4563, 126.7052),
    "ê´‘ì£¼": (35.1595, 126.8526),
    "ëŒ€ì „": (36.3504, 127.3845),
    "ìš¸ì‚°": (35.5384, 129.3114),
    "ì„¸ì¢…": (36.4800, 127.2890),
    "ê²½ê¸°": (37.4138, 127.5183),
    "ê°•ì›": (37.8228, 128.1555),
    "ì¶©ë¶": (36.6357, 127.4917),
    "ì¶©ë‚¨": (36.6588, 126.6728),
    "ì „ë¶": (35.7175, 127.1530),
    "ì „ë‚¨": (34.8161, 126.4629),
    "ê²½ë¶": (36.4919, 128.8889),
    "ê²½ë‚¨": (35.4606, 128.2132),
    "ì œì£¼": (33.4996, 126.5312),
}


# -----------------------------
# Population preprocessing
# -----------------------------
def preprocess_population(pop_raw: pd.DataFrame) -> pd.DataFrame:
    """
    ê¸°ëŒ€ ì»¬ëŸ¼(ì˜ˆ):
    - í–‰ì •êµ¬ì—­
    - 2025ë…„12ì›”_ì „ì›”ì¸êµ¬ìˆ˜_ê³„
    - 2025ë…„12ì›”_ë‹¹ì›”ì¸êµ¬ìˆ˜_ê³„
    - 2025ë…„12ì›”_ì¸êµ¬ì¦ê°_ê³„
    """
    df = pop_raw.copy()

    if "í–‰ì •êµ¬ì—­" not in df.columns:
        raise ValueError("ì¸êµ¬ ë°ì´í„°ì— 'í–‰ì •êµ¬ì—­' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    df["ì‹œë„"] = df["í–‰ì •êµ¬ì—­"].apply(normalize_sido_from_pop)

    # detect month prefix 'YYYYë…„Mì›”_'
    month_prefix = None
    for c in df.columns:
        m = re.match(r"(\d{4}ë…„\d{1,2}ì›”)_", str(c))
        if m:
            month_prefix = m.group(1)
            break
    if not month_prefix:
        raise ValueError("ì¸êµ¬ ë°ì´í„°ì—ì„œ 'YYYYë…„Mì›”_' í˜•íƒœì˜ ì»¬ëŸ¼ ì ‘ë‘ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    prev_col = f"{month_prefix}_ì „ì›”ì¸êµ¬ìˆ˜_ê³„"
    curr_col = f"{month_prefix}_ë‹¹ì›”ì¸êµ¬ìˆ˜_ê³„"
    diff_col = f"{month_prefix}_ì¸êµ¬ì¦ê°_ê³„"

    missing = [c for c in [prev_col, curr_col, diff_col] if c not in df.columns]
    if missing:
        raise ValueError(f"ì¸êµ¬ ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    df["ì „ì›”ì¸êµ¬"] = to_numeric_safe(df[prev_col])
    df["ë‹¹ì›”ì¸êµ¬"] = to_numeric_safe(df[curr_col])
    df["ì¸êµ¬ì¦ê°"] = to_numeric_safe(df[diff_col])
    df["ì¸êµ¬ì¦ê°ë¥ (%)"] = np.where(df["ì „ì›”ì¸êµ¬"] > 0, (df["ì¸êµ¬ì¦ê°"] / df["ì „ì›”ì¸êµ¬"]) * 100, np.nan)

    ym = re.match(r"(\d{4})ë…„(\d{1,2})ì›”", month_prefix)
    df["ì¸êµ¬ê¸°ì¤€ì—°ë„"] = int(ym.group(1)) if ym else np.nan
    df["ì¸êµ¬ê¸°ì¤€ì›”"] = int(ym.group(2)) if ym else np.nan

    # remove ì „êµ­ row if exists
    df = df[df["ì‹œë„"].notna()].copy()
    df = df[df["ì‹œë„"] != "ì „êµ­"].copy()

    # fallback lat/lon
    df["lat"] = df["ì‹œë„"].map(lambda x: SIDO_CENTROIDS.get(x, (np.nan, np.nan))[0])
    df["lon"] = df["ì‹œë„"].map(lambda x: SIDO_CENTROIDS.get(x, (np.nan, np.nan))[1])

    return df[["ì‹œë„", "ì „ì›”ì¸êµ¬", "ë‹¹ì›”ì¸êµ¬", "ì¸êµ¬ì¦ê°", "ì¸êµ¬ì¦ê°ë¥ (%)", "ì¸êµ¬ê¸°ì¤€ì—°ë„", "ì¸êµ¬ê¸°ì¤€ì›”", "lat", "lon"]]


# -----------------------------
# HIRA preprocessing & aggregation
# -----------------------------
def preprocess_hira(hira_raw: pd.DataFrame) -> pd.DataFrame:
    required = ["ì§„ë£Œë…„ë„", "ì‹œë„", "í–‰ìœ„ì½”ë“œ", "í™˜ììˆ˜", "ëª…ì„¸ì„œê±´ìˆ˜", "ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰", "ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡"]
    missing = [c for c in required if c not in hira_raw.columns]
    if missing:
        raise ValueError(f"ì‹¬í‰ì› ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    df = hira_raw.copy()
    for c in ["í™˜ììˆ˜", "ëª…ì„¸ì„œê±´ìˆ˜", "ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰", "ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡"]:
        df[c] = to_numeric_safe(df[c])

    df["ì§„ë£Œë…„ë„"] = to_numeric_safe(df["ì§„ë£Œë…„ë„"]).astype("Int64")
    df["ì‹œë„"] = df["ì‹œë„"].astype(str).str.strip()
    df["í–‰ìœ„ì½”ë“œ"] = df["í–‰ìœ„ì½”ë“œ"].astype(str).str.strip()
    return df


def aggregate_hira_by_sido(hira_df: pd.DataFrame, year: int, fillna_zero: bool = True) -> pd.DataFrame:
    df = hira_df[hira_df["ì§„ë£Œë…„ë„"] == year].copy()
    if fillna_zero:
        cols = ["í™˜ììˆ˜", "ëª…ì„¸ì„œê±´ìˆ˜", "ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰", "ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡"]
        df[cols] = df[cols].fillna(0)

    agg = df.groupby("ì‹œë„", as_index=False).agg(
        í™˜ììˆ˜=("í™˜ììˆ˜", "sum"),
        ëª…ì„¸ì„œê±´ìˆ˜=("ëª…ì„¸ì„œê±´ìˆ˜", "sum"),
        ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰=("ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰", "sum"),
        ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡=("ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡", "sum"),
        í–‰ìœ„ì½”ë“œì¢…ë¥˜ìˆ˜=("í–‰ìœ„ì½”ë“œ", "nunique"),
    )
    agg["ì§„ë£Œë…„ë„"] = year
    return agg


# -----------------------------
# UI
# -----------------------------
st.title("ğŸ“ ì§€ì—­ë³„ ì˜ë£Œí–‰ìœ„(ì‹¬í‰ì›) Ã— ì¸êµ¬ì¦ê°(ì£¼ë¯¼ë“±ë¡) ëŒ€ì‹œë³´ë“œ")

with st.sidebar:
    st.header("1) íŒŒì¼ ì„¤ì •")

    use_repo_files = st.checkbox("GitHub(ë ˆí¬) ë‚´ CSVë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©", value=True)

    hira_file = st.file_uploader("ì‹¬í‰ì› ì˜ë£Œí–‰ìœ„ CSV ì—…ë¡œë“œ(ì„ íƒ)", type=["csv"])
    pop_file = st.file_uploader("ì£¼ë¯¼ë“±ë¡ ì¸êµ¬ì¦ê° CSV ì—…ë¡œë“œ(ì„ íƒ)", type=["csv"])
# (ë ˆí¬ì— ë“¤ì–´ìˆëŠ” ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ) - ì‹¤ì œ íŒŒì¼ëª…ì— ë§ê²Œ ìˆ˜ì •!
DEFAULT_HIRA_PATH = "ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì›_ì˜ë£Œí–‰ìœ„ë³„ ì‹œë„ë³„ ê±´ê°•ë³´í—˜ ì§„ë£Œ í†µê³„_20241231.csv"
DEFAULT_POP_PATH  = "202512_202512_ì£¼ë¯¼ë“±ë¡ì¸êµ¬ê¸°íƒ€í˜„í™©(ì¸êµ¬ì¦ê°)_ì›”ê°„.csv"

try:
    if use_repo_files and (hira_file is None) and (pop_file is None):
        hira_raw = read_csv_from_path(DEFAULT_HIRA_PATH)
        pop_raw  = read_csv_from_path(DEFAULT_POP_PATH)
        st.sidebar.success("ë ˆí¬ ë‚´ ê¸°ë³¸ CSVë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        if (hira_file is None) or (pop_file is None):
            st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, 'ë ˆí¬ ë‚´ CSV ì‚¬ìš©'ì„ ì¼œì£¼ì„¸ìš”.")
            st.stop()

        hira_raw = read_csv_auto(hira_file)
        pop_raw  = read_csv_auto(pop_file)

except FileNotFoundError as e:
    st.error(
        "ë ˆí¬ ë‚´ CSV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
        "1) íŒŒì¼ëª…ì´ ì½”ë“œì˜ DEFAULT_*_PATHì™€ ë™ì¼í•œì§€\n"
        "2) íŒŒì¼ì´ app.pyì™€ ê°™ì€ í´ë”(ë˜ëŠ” ì§€ì •í•œ ê²½ë¡œ)ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\n\n"
        f"ì—ëŸ¬: {e}"
    )
    st.stop()
except Exception as e:
    st.error(f"CSV ë¡œë”© ì˜¤ë¥˜: {e}")
    st.stop()

# Load
try:
    hira_raw = read_csv_auto(hira_file)
    pop_raw = read_csv_auto(pop_file)
except Exception as e:
    st.error(f"CSV ë¡œë”© ì˜¤ë¥˜: {e}")
    st.stop()

# Preprocess
try:
    hira = preprocess_hira(hira_raw)
    pop = preprocess_population(pop_raw)
except Exception as e:
    st.error(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    st.stop()

# Year selector
years = sorted([int(y) for y in hira["ì§„ë£Œë…„ë„"].dropna().unique()])
if not years:
    st.error("ì‹¬í‰ì› ë°ì´í„°ì—ì„œ 'ì§„ë£Œë…„ë„'ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    st.stop()

default_year = years[-1]
left, right = st.columns([1, 2])
with left:
    year = st.selectbox("ì§„ë£Œë…„ë„ ì„ íƒ", options=years, index=years.index(default_year))
with right:
    st.caption("â€» ì¸êµ¬ ë°ì´í„°ëŠ” ì—…ë¡œë“œëœ íŒŒì¼ì˜ ì›”(ì „ì›”â†’ë‹¹ì›” ì¦ê°ë¥ )ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.")

# Aggregate & merge
hira_sido = aggregate_hira_by_sido(hira, year=year, fillna_zero=fillna_zero)
merged = hira_sido.merge(pop, on="ì‹œë„", how="left")

# Derived metrics
merged["ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰"] = np.where(merged["ë‹¹ì›”ì¸êµ¬"] > 0, (merged["ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰"] / merged["ë‹¹ì›”ì¸êµ¬"]) * 10000, np.nan)
merged["ì¸êµ¬1ë§Œëª…ë‹¹_ì²­êµ¬ê¸ˆì•¡"] = np.where(merged["ë‹¹ì›”ì¸êµ¬"] > 0, (merged["ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡"] / merged["ë‹¹ì›”ì¸êµ¬"]) * 10000, np.nan)
merged["í™˜ìë‹¹_ì²­êµ¬ê¸ˆì•¡"] = np.where(merged["í™˜ììˆ˜"] > 0, merged["ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡"] / merged["í™˜ììˆ˜"], np.nan)

national_avg = np.nanmean(merged["ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰"])
merged["í‘œì¤€í™”ì§€ìˆ˜(ì´ì‚¬ìš©ëŸ‰)"] = (
    merged["ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰"] / national_avg
    if national_avg and not np.isnan(national_avg)
    else np.nan
)

# Map id for choropleth
merged["geo_id"] = merged["ì‹œë„"].map(GEO_ID_BY_SIDO)

# Filters
all_sidos = [s for s in merged["ì‹œë„"].dropna().unique().tolist()]
sel_sidos = st.multiselect("í‘œì‹œí•  ì‹œë„ ì„ íƒ(ë¯¸ì„ íƒ ì‹œ ì „ì²´)", options=all_sidos, default=all_sidos)
view = merged[merged["ì‹œë„"].isin(sel_sidos)].copy()

# KPI
k1, k2, k3, k4 = st.columns(4)
k1.metric("ì‹œë„ ìˆ˜", f"{view['ì‹œë„'].nunique()}ê°œ")
k2.metric("ì˜ë£Œí–‰ìœ„ ì²­êµ¬ê¸ˆì•¡ í•©ê³„", f"{view['ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡'].sum():,.0f}")
k3.metric("ì¸êµ¬ 1ë§Œëª…ë‹¹ ì´ì‚¬ìš©ëŸ‰(í‰ê· )", f"{np.nanmean(view['ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰']):,.2f}")
k4.metric("ì¸êµ¬ ì¦ê°ë¥ (í‰ê· , %)", f"{np.nanmean(view['ì¸êµ¬ì¦ê°ë¥ (%)']):,.3f}")

st.divider()

tab1, tab2, tab3 = st.tabs(["ğŸ«§ ë²„ë¸”(ì¸êµ¬ì¦ê° Ã— ì˜ë£Œì´ìš©)", "ğŸ—ºï¸ ì‹œë„ ê²½ê³„ ì§€ë„(Choropleth)", "ğŸ“‹ ë­í‚¹/í…Œì´ë¸”"])

# -----------------------------
# Tab 1: Bubble
# -----------------------------
with tab1:
    metric_choice = st.radio(
        "Yì¶• ì§€í‘œ ì„ íƒ",
        ["ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰", "ì¸êµ¬1ë§Œëª…ë‹¹_ì²­êµ¬ê¸ˆì•¡", "í™˜ìë‹¹_ì²­êµ¬ê¸ˆì•¡"],
        horizontal=True,
    )

    bubble = view.copy()
    bubble["ë²„ë¸”í¬ê¸°(ì¸êµ¬)"] = bubble["ë‹¹ì›”ì¸êµ¬"]
    bubble["ë¼ë²¨"] = bubble["ì‹œë„"]

    fig = px.scatter(
        bubble,
        x="ì¸êµ¬ì¦ê°ë¥ (%)",
        y=metric_choice,
        size="ë²„ë¸”í¬ê¸°(ì¸êµ¬)",
        hover_name="ë¼ë²¨",
        hover_data={
            "ë‹¹ì›”ì¸êµ¬": ":,",
            "ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰": ":,",
            "ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡": ":,",
            "ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰": ":.2f",
            "ì¸êµ¬1ë§Œëª…ë‹¹_ì²­êµ¬ê¸ˆì•¡": ":.2f",
            "í™˜ìë‹¹_ì²­êµ¬ê¸ˆì•¡": ":.2f",
            "í‘œì¤€í™”ì§€ìˆ˜(ì´ì‚¬ìš©ëŸ‰)": ":.2f",
        },
        labels={
            "ì¸êµ¬ì¦ê°ë¥ (%)": "ì¸êµ¬ ì¦ê°ë¥ (%) (ì „ì›”â†’ë‹¹ì›”)",
            metric_choice: metric_choice,
        },
        title="ì¸êµ¬ ë³€í™” vs ì˜ë£Œì´ìš©(ì¸êµ¬ë³´ì •) â€” ë²„ë¸” í¬ê¸°=ì¸êµ¬",
    )
    st.plotly_chart(fig, use_container_width=True)

    st.caption("í•´ì„ íŒ: ì¢Œìƒë‹¨(ì¸êµ¬â†“, ì˜ë£Œì´ìš©â†‘)ì€ ê³ ë ¹í™”/ë§Œì„±ì§ˆí™˜/ê³µê¸‰ êµ¬ì¡° ë“±ì˜ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•  ìˆ˜ ìˆì–´ìš”.")

# -----------------------------
# Tab 2: Choropleth map (no geojson file needed)
# -----------------------------
 # -----------------------------
# Tab 2: Map
# -----------------------------
with tab2:
    st.subheader("ğŸ—ºï¸ ì§€ë„")

    safe_mode = st.checkbox("ì§€ë„ ì•ˆì „ëª¨ë“œ(ì™¸ë¶€ GeoJSON ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨)", value=True)

    map_metric = st.selectbox(
        "ì§€ë„ì—ì„œ ìƒ‰ìœ¼ë¡œ í‘œí˜„í•  ì§€í‘œ",
        ["ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰", "ì¸êµ¬1ë§Œëª…ë‹¹_ì²­êµ¬ê¸ˆì•¡", "ì¸êµ¬ì¦ê°ë¥ (%)", "í‘œì¤€í™”ì§€ìˆ˜(ì´ì‚¬ìš©ëŸ‰)"],
    )

    map_df = view.copy()

    if safe_mode:
        fallback = map_df.copy()
        fallback["lat"] = fallback["ì‹œë„"].map(lambda x: SIDO_CENTROIDS.get(x, (np.nan, np.nan))[0])
        fallback["lon"] = fallback["ì‹œë„"].map(lambda x: SIDO_CENTROIDS.get(x, (np.nan, np.nan))[1])
        fallback = fallback.dropna(subset=["lat", "lon"])

        fig2 = px.scatter_mapbox(
            fallback,
            lat="lat",
            lon="lon",
            size="ë‹¹ì›”ì¸êµ¬",
            color=map_metric,
            hover_name="ì‹œë„",
            zoom=5.5,
            height=650,
        )
        fig2.update_layout(
            mapbox_style="open-street-map",
            margin=dict(l=0, r=0, t=0, b=0),
        )
        st.plotly_chart(fig2, use_container_width=True)
        st.stop()

# -----------------------------
# Tab 3: Table / ranking
# -----------------------------
with tab3:
    rank_metric = st.selectbox(
        "ë­í‚¹ ê¸°ì¤€",
        ["ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡", "ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰", "ëª…ì„¸ì„œê±´ìˆ˜", "í™˜ììˆ˜",
         "ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰", "ì¸êµ¬1ë§Œëª…ë‹¹_ì²­êµ¬ê¸ˆì•¡", "í™˜ìë‹¹_ì²­êµ¬ê¸ˆì•¡"],
    )

    ranked = view.sort_values(rank_metric, ascending=False).copy()

    st.subheader(f"Top {top_n} ì‹œë„ â€” {rank_metric}")

    cols = [
        "ì‹œë„", "ì§„ë£Œë…„ë„",
        "ë‹¹ì›”ì¸êµ¬", "ì¸êµ¬ì¦ê°", "ì¸êµ¬ì¦ê°ë¥ (%)",
        "í™˜ììˆ˜", "ëª…ì„¸ì„œê±´ìˆ˜", "ì˜ë£Œí–‰ìœ„ì´ì‚¬ìš©ëŸ‰", "ì˜ë£Œí–‰ìœ„ì²­êµ¬ê¸ˆì•¡",
        "ì¸êµ¬1ë§Œëª…ë‹¹_ì´ì‚¬ìš©ëŸ‰", "ì¸êµ¬1ë§Œëª…ë‹¹_ì²­êµ¬ê¸ˆì•¡", "í™˜ìë‹¹_ì²­êµ¬ê¸ˆì•¡",
        "í–‰ìœ„ì½”ë“œì¢…ë¥˜ìˆ˜", "í‘œì¤€í™”ì§€ìˆ˜(ì´ì‚¬ìš©ëŸ‰)"
    ]
    st.dataframe(ranked[cols].head(top_n), use_container_width=True)

    csv = ranked[cols].to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ì§‘ê³„ í…Œì´ë¸” CSV ë‹¤ìš´ë¡œë“œ",
        data=csv,
        file_name=f"merged_hira_pop_{year}.csv",
        mime="text/csv",
    )

st.caption("â“˜ í™˜ììˆ˜ëŠ” í–‰ìœ„ì½”ë“œë³„ ì¤‘ë³µ ì§‘ê³„ ê°€ëŠ¥ì„±ì´ ìˆì–´, ì§€ì—­ ë¹„êµëŠ” 'ì´ì‚¬ìš©ëŸ‰/ì²­êµ¬ê¸ˆì•¡/ì¸êµ¬ë³´ì • ì§€í‘œ' ì¤‘ì‹¬ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
